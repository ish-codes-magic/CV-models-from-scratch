{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c9b06f",
   "metadata": {},
   "source": [
    "## Alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b1231",
   "metadata": {},
   "source": [
    "AlexNet is an Image Classification model that transformed deep learning. It was introduced by Geoffrey Hinton and his team in 2012 and marked a key event in the history of deep learning, showcasing the strengths of CNN architectures and its vast applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d227f1",
   "metadata": {},
   "source": [
    "### Before Alexnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4edd4a6",
   "metadata": {},
   "source": [
    "Machine learning models such as Support Vector Machines (SVMs) and shallow neural networks dominated computer vision before the development of AlexNet. Training a deep model with millions of parameters seemed impossible, we will investigate why, but first, let’s look into the limitations of the previous Machine Learning (ML) models.\n",
    "\n",
    "* **Feature Engineering**: SVMs and simple Neural Networks (NNs) require extensive handcrafted feature engineering, which makes scaling and generalization impossible.\n",
    "* **Computational Resources**: Before AlexNet, researchers primarily used CPUs to train models because they did not have direct access to GPU processing. This changed when Nvidia released the CUDA API, allowing AI software to access parallel processing using GPUs.\n",
    "* **Vanishing Gradient Problem**: Deep Networks faced a vanishing gradient problem. This is where the gradients become too small during backpropagation or disappear completely.\n",
    "\n",
    "Due to computational limitations, gradient vanishing, and a lack of large datasets to train the model on, most neural networks were shallow. These obstacles made it impossible for the model to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0eb8d0",
   "metadata": {},
   "source": [
    "### Contribution of AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980554a",
   "metadata": {},
   "source": [
    "\n",
    "The AlexNet paper titled “ImageNet Classification with Deep Convolutional Neural Networks” solved the above-discussed problems.\n",
    "\n",
    "This paper’s release deeply influenced the trajectory of deep learning. The methods and innovations introduced became a standard for training Deep Neural Networks. Here are the key innovations introduced:\n",
    "\n",
    "* **Deep Architecture**: This model utilized deep architecture compared to any NN model released previously. It consisted of five convolutional layers followed by three fully connected layers.\n",
    "* **ReLU Nonlinearity**: CNNs at that time used functions such as Tanh or Sigmoid to process information between layers. These functions slowed down the training. In contrast, ReLU (Rectified Linear Unit) made the entire process simpler and many times faster. It outputs only if the input is given to it as positive, otherwise, it outputs a zero.\n",
    "* **Overlapping Pooling**: Overlapping pooling is just like regular max pooling layers, but in overlapping pooling, as the window moves across, it overlaps with the previous window. This improved the error percentage in AlexNet.\n",
    "* **Use of GPU**: Before AlexNet, NNs were trained on the CPU, which made the process slow. However, the researcher of AlexNet incorporated GPUs, which accelerated computation time significantly. This proved that Deep NNs can be trained feasibly on GPUs.\n",
    "* **Local Response Normalization (LRN)**: This is a process of normalizing adjacent channels in the network, which normalizes the activity of neurons within a local neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6da233",
   "metadata": {},
   "source": [
    "### The AlexNet Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1efdbf",
   "metadata": {},
   "source": [
    "![Alexnet architecture](./../img/alexNet-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69562ac4",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110d67c",
   "metadata": {},
   "source": [
    "AlexNet takes images of the Input size of 227x227x3 RGB Pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0824b",
   "metadata": {},
   "source": [
    "### Convolutional Layers\n",
    "* **First Layer**: The first layer uses 96 kernels of size 11×11 with a stride of 4, activates them with the ReLU activation function, and then performs a Max Pooling operation.\n",
    "* **Second Layer**: The second layer takes the output of the first layer as the input, with 256 kernels of size 5x5x48.\n",
    "* **Third Layer**: 384 kernels of size 3x3x256. No pooling or normalization operations are performed on the third, fourth, and fifth layers.\n",
    "* **Fourth Layer**: 384 kernels of size 3x3x192.\n",
    "* **Fifth Layer**: 256 kernels of size 3x3x192."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af742031",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "The output layer is a SoftMax layer that outputs probabilities of the 1000 class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c2a98a",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "AlexNet uses stochastic gradient descent (SGD) with momentum for optimization. \n",
    "\n",
    "SGD is a variant of the traditional gradient descent algorithm but offers several advantages in terms of efficiency and scalability making it the go-to method for many deep-learning tasks.\n",
    "\n",
    "* In traditional gradient descent, the gradients are computed based on the entire dataset which can be computationally expensive for large datasets.\n",
    "* In Stochastic Gradient Descent, the gradient is calculated for each training example (or a small subset of training examples) rather than the entire dataset.\n",
    "\n",
    "![SGD](./../img/stochastic.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9a20a",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "Momentum helps accelerate SGD and dampens fluctuations. It performs this by adding a fraction of the previous weight vector to the current weight vector. This prevents sharp updates and helps the model overcome saddle points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f9e9a9",
   "metadata": {},
   "source": [
    "Formally, the weight update rule with momentum can be written as:\n",
    "\n",
    "\n",
    "Δw:=αΔw−η∇Q \n",
    "\n",
    "w:=w+Δw\n",
    "\n",
    "where \n",
    "\n",
    "w represents the parameters, \n",
    "\n",
    "η is the learning rate, \n",
    "\n",
    "α is the momentum coefficient (between 0 and 1), and \n",
    "\n",
    "∇$Q_{i}$(w) is the gradient of the loss for a given sample. \n",
    "\n",
    "The term αΔw carries forward part of the previous update, which helps accelerate movement in consistent directions and reduces oscillations in the optimization trajectory. This concept is analogous to momentum in physics and helps the algorithm avoid getting stuck in local minima and navigate ravines more efficiently.\n",
    "\n",
    "Momentum typically improves convergence speed and can lead to more robust training, especially in deep neural networks. A common choice for the momentum hyperparameter is around 0.9, though it can be tuned depending on the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629decc",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "The paper introduce the ReLU activation function.\n",
    "\n",
    "In simpler terms, ReLU allows positive values to pass through unchanged while setting all negative values to zero. This helps the neural network maintain the necessary complexity to learn patterns while avoiding some of the pitfalls associated with other activation functions, like the vanishing gradient problem.\n",
    "\n",
    "The ReLU function can be described mathematically as follows:\n",
    "\n",
    "f(x)=max(0,x)\n",
    "\n",
    "Where:\n",
    "\n",
    "x is the input to the neuron.\n",
    "\n",
    "The function returns x if x is greater than 0.\n",
    "If x is less than or equal to 0, the function returns 0.\n",
    "\n",
    "\n",
    "This simplicity is what makes ReLU so effective in training deep neural networks, as it helps to maintain non-linearity without complicated transformations, allowing models to learn more efficiently.\n",
    "\n",
    "If we plot the graph of ReLU activation function, it will appear like this:\n",
    "\n",
    "![Relu](./../img/Relu-activation-function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6148eaf",
   "metadata": {},
   "source": [
    "### Dropout layers\n",
    "\n",
    "Dropout is a technique to improve the overfitting problem of Neural Network that was introduced in the paper.\n",
    "\n",
    "The term \"dropout\" refers to dropping out the nodes (input and hidden layer) in a neural network. All the forward and backwards connections with a dropped node are temporarily removed, thus creating a new network architecture out of the parent network. The nodes are dropped by a dropout probability of p.\n",
    "\n",
    "In the overfitting problem, the model learns the statistical noise. To be precise, the main motive of training is to decrease the loss function, given all the units (neurons). So in overfitting, a unit may change in a way that fixes up the mistakes of the other units. This leads to complex co-adaptations, which in turn leads to the overfitting problem because this complex co-adaptation fails to generalise on the unseen dataset.\n",
    "\n",
    "Now, if we use dropout, it prevents these units to fix up the mistake of other units, thus preventing co-adaptation, as in every iteration the presence of a unit is highly unreliable. So by randomly dropping a few units (nodes), it forces the layers to take more or less responsibility for the input by taking a probabilistic approach.\n",
    "\n",
    "This ensures that the model is getting generalised and hence reducing the overfitting problem.\n",
    "\n",
    "![droput](./../img/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e214252",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f5737",
   "metadata": {},
   "source": [
    "We will be using the CIFAR-10 dataset. The dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random sample images from each:\n",
    "\n",
    "![cifar10](./../img/CIFAR-10.png)\n",
    "\n",
    "The classes are completely mutually exclusive, with no overlap between automobiles and trucks. “Automobile” encompasses sedans, SUVs, and similar vehicles, while “Truck” refers solely to large trucks, excluding pickup trucks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3573b9e6",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7515888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820424d",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b57869",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "batch_size = 64\n",
    "random_seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95abb4d",
   "metadata": {},
   "source": [
    "### Transforming the data\n",
    "\n",
    "We first transform the data according to the Alexnet in the following steps:\n",
    "\n",
    "* `Resize` - resizes images to 227×227\n",
    "* `ToTensor` converts a PIL image in [0,255] to a tensor in [0,1] (C×H×W)\n",
    "* `Normalize`: shifts pixel intensities channel-wise using (x - mean) / std.\n",
    "\n",
    "We get the mean and std values of the train and std data directly from the CIFAR-10 data which is well known, so we use those values directly, otherwise you can also calculate the mean and std of the entire train and test dataset and apply the normalise function directly on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6acd799",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std  = [0.2023, 0.1994, 0.2010]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227,227)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f705196",
   "metadata": {},
   "source": [
    "This transform will convert the raw PIL image which is in numpy array to standardise to 227×227×3 arrays which are then converted to Tensors and normalised to be [0,1] range values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8149447d",
   "metadata": {},
   "source": [
    "Let's see, how the transforms happen one by one. First we will load the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "151220a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = datasets.CIFAR10(\n",
    "    root=data_dir, train=True,\n",
    "    download=True, transform=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30965b7",
   "metadata": {},
   "source": [
    "Now, we inspect one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1d6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img, label = raw_dataset[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eca83d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(pil_img)\n",
    "plt.title(\"Raw PIL image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64885be4",
   "metadata": {},
   "source": [
    "As we can see below the PIL image is 32×32 with 3 channels and each value in the tensor is uint8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac80c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode/size: RGB (32, 32)\n",
      "Raw array dtype: uint8 shape: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Mode/size:\", pil_img.mode, pil_img.size)\n",
    "raw_np = np.array(pil_img)  # H x W x C, uint8\n",
    "print(\"Raw array dtype:\", raw_np.dtype, \"shape:\", raw_np.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc4632",
   "metadata": {},
   "source": [
    "After applying the first transform the 32×32 with 3 channel PIL image gets converted into 227×227 with 3 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ec7793a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed: Resize(size=(227, 227), interpolation=bilinear, max_size=None, antialias=True)\n"
     ]
    }
   ],
   "source": [
    "transformed = transform.transforms[0]\n",
    "print(\"Transformed:\", transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6f4e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_img = F.resize(pil_img, transformed.size, interpolation=transformed.interpolation, max_size=transformed.max_size, antialias=transformed.antialias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "184a28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: PIL.Image, dtype: uint8 shape: (227, 227, 3)\n"
     ]
    }
   ],
   "source": [
    "np_resized = np.array(resized_img)\n",
    "print(\"Type: PIL.Image, dtype:\", np_resized.dtype, \"shape:\", np_resized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37462484",
   "metadata": {},
   "source": [
    "After this we convert into the tensor format which converts in into 3×227×227 tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "448ae679",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_img = F.to_tensor(resized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8df30985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.Tensor, dtype: torch.float32 shape: (3, 227, 227)\n"
     ]
    }
   ],
   "source": [
    "print(\"Type: torch.Tensor, dtype:\", tensor_img.dtype, \"shape:\", tuple(tensor_img.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3c07ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed: Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])\n"
     ]
    }
   ],
   "source": [
    "transformed = transform.transforms[2]\n",
    "print(\"Transformed:\", transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dc3affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tensor = F.normalize(tensor_img, mean=transformed.mean, std=transformed.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79b465e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.Tensor, dtype: torch.float32 shape: (3, 227, 227)\n"
     ]
    }
   ],
   "source": [
    "print(\"Type: torch.Tensor, dtype:\", final_tensor.dtype, \"shape:\", tuple(final_tensor.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2c1b89",
   "metadata": {},
   "source": [
    "We can wrap this entire transform as well as splitting the training data into train and validation set into 2 functions and load the data directly in the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c660084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_loader(data_dir,\n",
    "                            batch_size,\n",
    "                            random_seed,\n",
    "                            valid_size=0.1,\n",
    "                            shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "def get_test_loader(data_dir,\n",
    "                    batch_size,\n",
    "                    shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "\n",
    "    # define transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((227,227)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=False,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c65aa329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 dataset \n",
    "train_loader, valid_loader = get_train_valid_loader(data_dir = './data',batch_size = 64,random_seed = 1)\n",
    "\n",
    "test_loader = get_test_loader(data_dir = './data',\n",
    "                                batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd5d9d",
   "metadata": {},
   "source": [
    "## Implementing Alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b2fb5",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "Useful formula for conv/pool output size per spatial dimension:\n",
    "\n",
    "\n",
    "out=⌊in+2p−k/s⌋+1\n",
    "\n",
    "where \n",
    "k=kernel, \n",
    "s=stride, \n",
    "p=padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ab958e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Layer 1 -----\n",
    "layer_1 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0),\n",
    "    nn.BatchNorm2d(96),\n",
    "    nn.ReLU(inplace=False),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280f49a",
   "metadata": {},
   "source": [
    "What Layer 1 does (Conv11 / s4 + BN + ReLU + Pool):\n",
    "\n",
    "Conv2d(3→96, k=11, s=4, p=0) on (N,3,227,227) → spatial:\n",
    "(227−11)/4+1=216/4+1=54+1=55 ⇒ (N, 96, 55, 55)\n",
    "\n",
    "BatchNorm2d(96): per-channel affine normalization; stabilizes training (modern replacement for AlexNet’s LRN).\n",
    "\n",
    "ReLU: nonlinearity.\n",
    "\n",
    "MaxPool2d(k=3, s=2): \n",
    "(55−3)/2+1=52/2+1=26+1=27 ⇒ (N, 96, 27, 27)\n",
    "\n",
    "Params (trainable):\n",
    "\n",
    "Conv: \n",
    "96×(3×11×11)+96 = 96×363+96= 34,944\n",
    "\n",
    "BN: \n",
    "2×96=192 (γ & β; running stats are buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6136fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Layer 2 -----\n",
    "layer_2 = nn.Sequential(\n",
    "    nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(inplace=False),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbfa146",
   "metadata": {},
   "source": [
    "What Layer 2 does (Conv5 / s1 p2 + BN + ReLU + Pool):\n",
    "\n",
    "Input: (N, 96, 27, 27)\n",
    "\n",
    "Conv2d(96→256, k=5, s=1, p=2) keeps size: (N, 256, 27, 27)\n",
    "\n",
    "MaxPool2d(k=3, s=2): (27−3)/2+1=24/2+1=12+1=13 ⇒ (N, 256, 13, 13)\n",
    "\n",
    "Params:\n",
    "\n",
    "Conv: 256×(96×5×5)+256=256×2400+256=614,656\n",
    "\n",
    "BN: 2×256=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b17e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Layer 3 -----\n",
    "layer_3 = nn.Sequential(\n",
    "    nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(384),\n",
    "    nn.ReLU(inplace=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4901c",
   "metadata": {},
   "source": [
    "What Layer 3 does (Conv3 / s1 p1 + BN + ReLU):\n",
    "\n",
    "Input: (N, 256, 13, 13)\n",
    "\n",
    "Conv2d(256→384, k=3, s=1, p=1) keeps size: (N, 384, 13, 13)\n",
    "\n",
    "Params:\n",
    "\n",
    "Conv: 384×(256×3×3)+384=384×2304+384=885,120\n",
    "\n",
    "BN: 2×384=768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "528f9763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Layer 4 -----\n",
    "layer_4 = nn.Sequential(\n",
    "    nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(384),\n",
    "    nn.ReLU(inplace=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb123e",
   "metadata": {},
   "source": [
    "What Layer 4 does (another Conv3 / s1 p1 + BN + ReLU):\n",
    "\n",
    "Input: (N, 384, 13, 13)\n",
    "\n",
    "Output: (N, 384, 13, 13)\n",
    "\n",
    "Params:\n",
    "\n",
    "Conv: 384×(384×3×3)+384=1,327,488\n",
    "\n",
    "BN: 2×384=768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88a81eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Layer 5 -----\n",
    "layer_5 = nn.Sequential(\n",
    "    nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(inplace=False),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab39597",
   "metadata": {},
   "source": [
    "What Layer 5 does (Conv3 / s1 p1 + BN + ReLU + Pool):\n",
    "\n",
    "Input: (N, 384, 13, 13)\n",
    "\n",
    "Conv2d(384→256, k=3, s=1, p=1) keeps size: (N, 256, 13, 13)\n",
    "\n",
    "MaxPool2d(k=3, s=2): (13−3)/2+1=10/2+1=5+1=6 ⇒ (N, 256, 6, 6)\n",
    "\n",
    "Params:\n",
    "\n",
    "Conv: 256×(384×3×3)+256=884,992\n",
    "\n",
    "BN: 2×256=512\n",
    "\n",
    "Flatten size after Layer 5 = 256×6×6=9216."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5aef136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- FC block A -----\n",
    "fc_a = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=9216, out_features=4096),\n",
    "    nn.ReLU(inplace=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7bc1a4",
   "metadata": {},
   "source": [
    "What FC A does (Dropout + Linear 9216→4096 + ReLU):\n",
    "\n",
    "Input: (N, 9216) after flatten\n",
    "\n",
    "Dropout(0.5): regularization, randomly zeroes ~50% activations during training\n",
    "\n",
    "Linear: (N, 4096)\n",
    "\n",
    "Params: 9216×4096+4096=37,752,832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28213a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- FC block B -----\n",
    "fc_b = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(inplace=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf353b4d",
   "metadata": {},
   "source": [
    "What FC B does (Dropout + Linear 4096→4096 + ReLU):\n",
    "\n",
    "Input/Output: (N, 4096)\n",
    "\n",
    "Params: 4096×4096+4096=16,781,312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50da2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Classifier -----\n",
    "fc_classifier = nn.Sequential(\n",
    "    nn.Linear(4096, 10)  # num_classes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e336d",
   "metadata": {},
   "source": [
    "What the classifier does (Linear 4096→10):\n",
    "\n",
    "Output: (N, 10) logits (for CIFAR-10)\n",
    "\n",
    "Params: 4096×10+10=40,970"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2b800",
   "metadata": {},
   "source": [
    "### Putting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dfa869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        # Convolutional feature extractor\n",
    "        self.layer1 = layer_1\n",
    "        self.layer2 = layer_2\n",
    "        self.layer3 = layer_3\n",
    "        self.layer4 = layer_4\n",
    "        self.layer5 = layer_5\n",
    "\n",
    "        # Fully-connected classifier head\n",
    "        # Note: change the out_features of the final layer if num_classes differs\n",
    "        self.fc  = fc_a\n",
    "        self.fc1 = fc_b\n",
    "        # replace last linear to match num_classes if needed\n",
    "        self.fc2 = nn.Sequential(nn.Linear(4096, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, 3, 227, 227)\n",
    "        out = self.layer1(x)  # -> (N, 96, 27, 27)\n",
    "        out = self.layer2(out)  # -> (N, 256, 13, 13)\n",
    "        out = self.layer3(out)  # -> (N, 384, 13, 13)\n",
    "        out = self.layer4(out)  # -> (N, 384, 13, 13)\n",
    "        out = self.layer5(out)  # -> (N, 256, 6, 6)\n",
    "\n",
    "        # Flatten\n",
    "        out = out.reshape(out.size(0), -1)  # -> (N, 9216)\n",
    "\n",
    "        # FC head\n",
    "        out = self.fc(out)   # -> (N, 4096)\n",
    "        out = self.fc1(out)  # -> (N, 4096)\n",
    "        out = self.fc2(out)  # -> (N, num_classes)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e9df2c",
   "metadata": {},
   "source": [
    "Differences from the original Alexnet Implementation:\n",
    "\n",
    "* **BatchNorm instead of LRN**\n",
    "AlexNet used Local Response Normalization (LRN). Modern practice uses BatchNorm, which usually works better and simplifies tuning.\n",
    "\n",
    "* **No group convolutions**\n",
    "Original AlexNet split some conv layers across two GPUs using “groups”. This model uses standard (ungrouped) convs.\n",
    "\n",
    "* **Input resolution**\n",
    "AlexNet’s canonical training used 227×227 (sometimes 224×224 in modern code—both fine with adjusted first conv). Your design expects 227×227 so that the flatten size becomes exactly 9216.\n",
    "\n",
    "* **Framework conveniences**\n",
    "PyTorch nn.Sequential makes the pipeline concise; ReLUs are standard; inplace not required here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bddbe35",
   "metadata": {},
   "source": [
    "### Setting up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd5d71a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "total_param = 623786344\n",
    "\n",
    "model = AlexNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e12b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs will be saved to: runs/alexnet_cifar10_20251005_200136\n",
      "To view TensorBoard, run: tensorboard --logdir=runs\n"
     ]
    }
   ],
   "source": [
    "# Create TensorBoard log directory\n",
    "log_dir = f\"runs/alexnet_cifar10_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Metrics storage for Plotly visualizations\n",
    "training_metrics = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'learning_rate': [],\n",
    "    'epoch_time': [],\n",
    "    'gradient_norm': [],\n",
    "    'weight_norm': []\n",
    "}\n",
    "\n",
    "# Class names for CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    \"\"\"Calculate accuracy from outputs and labels\"\"\"\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return correct / labels.size(0)\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    \"\"\"Calculate the norm of gradients\"\"\"\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** (1. / 2)\n",
    "\n",
    "def get_weight_norm(model):\n",
    "    \"\"\"Calculate the norm of weights\"\"\"\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** (1. / 2)\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "print(\"To view TensorBoard, run: tensorboard --logdir=runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb66d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Calculate gradient norm before optimizer step\n",
    "        grad_norm = get_gradient_norm(model)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += (torch.max(outputs, 1)[1] == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Log to TensorBoard every 100 steps\n",
    "        if (i + 1) % 100 == 0:\n",
    "            writer.add_scalar('Training/Loss_Step', loss.item(), global_step)\n",
    "            writer.add_scalar('Training/Accuracy_Step', \n",
    "                            calculate_accuracy(outputs, labels), global_step)\n",
    "            writer.add_scalar('Training/Gradient_Norm', grad_norm, global_step)\n",
    "            writer.add_scalar('Training/Weight_Norm', get_weight_norm(model), global_step)\n",
    "            \n",
    "            # Log learning rate\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            writer.add_scalar('Training/Learning_Rate', current_lr, global_step)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], '\n",
    "                  f'Loss: {loss.item():.4f}, Acc: {calculate_accuracy(outputs, labels):.4f}')\n",
    "        \n",
    "        global_step += 1\n",
    "\n",
    "    # Calculate epoch training metrics\n",
    "    epoch_train_loss = running_loss / total_samples\n",
    "    epoch_train_acc = running_corrects / total_samples\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "    val_total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_running_corrects += (predicted == labels).sum().item()\n",
    "            val_total_samples += labels.size(0)\n",
    "            \n",
    "            # Store predictions for confusion matrix\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate epoch validation metrics\n",
    "    epoch_val_loss = val_running_loss / val_total_samples\n",
    "    epoch_val_acc = val_running_corrects / val_total_samples\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Store metrics for Plotly visualization\n",
    "    training_metrics['epoch'].append(epoch + 1)\n",
    "    training_metrics['train_loss'].append(epoch_train_loss)\n",
    "    training_metrics['train_acc'].append(epoch_train_acc)\n",
    "    training_metrics['val_loss'].append(epoch_val_loss)\n",
    "    training_metrics['val_acc'].append(epoch_val_acc)\n",
    "    training_metrics['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "    training_metrics['epoch_time'].append(epoch_time)\n",
    "    training_metrics['gradient_norm'].append(grad_norm)\n",
    "    training_metrics['weight_norm'].append(get_weight_norm(model))\n",
    "    \n",
    "    # Log epoch metrics to TensorBoard\n",
    "    writer.add_scalar('Epoch/Train_Loss', epoch_train_loss, epoch)\n",
    "    writer.add_scalar('Epoch/Train_Accuracy', epoch_train_acc, epoch)\n",
    "    writer.add_scalar('Epoch/Val_Loss', epoch_val_loss, epoch)\n",
    "    writer.add_scalar('Epoch/Val_Accuracy', epoch_val_acc, epoch)\n",
    "    writer.add_scalar('Epoch/Time', epoch_time, epoch)\n",
    "    \n",
    "    # Log confusion matrix every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        cm = confusion_matrix(all_labels, all_predictions)\n",
    "        fig_cm = px.imshow(cm, \n",
    "                          x=class_names, \n",
    "                          y=class_names,\n",
    "                          color_continuous_scale='Blues',\n",
    "                          title=f'Confusion Matrix - Epoch {epoch+1}')\n",
    "        writer.add_figure(f'Confusion_Matrix/Epoch_{epoch+1}', fig_cm.to_image(format=\"png\"), epoch)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
    "          f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, '\n",
    "          f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}, '\n",
    "          f'Time: {epoch_time:.2f}s')\n",
    "    \n",
    "# Log model graph\n",
    "dummy_input = torch.randn(1, 3, 227, 227).to(device)\n",
    "writer.add_graph(model, dummy_input)\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "print(f\"Training completed! TensorBoard logs saved to: {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086115d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Plotly Visualizations\n",
    "\n",
    "def create_training_dashboard():\n",
    "    \"\"\"Create comprehensive training dashboard with multiple plots\"\"\"\n",
    "    \n",
    "    # Create subplot figure\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=('Loss Curves', 'Accuracy Curves', \n",
    "                       'Learning Rate Schedule', 'Training Time per Epoch',\n",
    "                       'Gradient & Weight Norms', 'Loss vs Accuracy Correlation'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": True}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    epochs = training_metrics['epoch']\n",
    "    \n",
    "    # 1. Loss Curves\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=training_metrics['train_loss'], \n",
    "                  name='Training Loss', line=dict(color='red')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=training_metrics['val_loss'], \n",
    "                  name='Validation Loss', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Accuracy Curves\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=training_metrics['train_acc'], \n",
    "                  name='Training Accuracy', line=dict(color='green')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=training_metrics['val_acc'], \n",
    "                  name='Validation Accuracy', line=dict(color='orange')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Learning Rate Schedule\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=training_metrics['learning_rate'], \n",
    "                  name='Learning Rate', line=dict(color='purple')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Training Time per Epoch\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=epochs, y=training_metrics['epoch_time'], \n",
    "               name='Epoch Time', marker_color='lightblue'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 5. Gradient & Weight Norms\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=training_metrics['gradient_norm'], \n",
    "                  name='Gradient Norm', line=dict(color='red')),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=training_metrics['weight_norm'], \n",
    "                  name='Weight Norm', line=dict(color='blue')),\n",
    "        row=3, col=1, secondary_y=True\n",
    "    )\n",
    "    \n",
    "    # 6. Loss vs Accuracy Correlation\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=training_metrics['val_loss'], y=training_metrics['val_acc'],\n",
    "                  mode='markers+lines', name='Val Loss vs Acc',\n",
    "                  marker=dict(size=8, color=epochs, colorscale='Viridis'),\n",
    "                  text=[f'Epoch {e}' for e in epochs]),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=1200,\n",
    "        title_text=\"AlexNet Training Dashboard\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update y-axis titles\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Learning Rate\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Time (seconds)\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Gradient Norm\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Weight Norm\", row=3, col=1, secondary_y=True)\n",
    "    fig.update_yaxes(title_text=\"Validation Accuracy\", row=3, col=2)\n",
    "    fig.update_xaxes(title_text=\"Validation Loss\", row=3, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display the dashboard\n",
    "dashboard_fig = create_training_dashboard()\n",
    "dashboard_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Performance Analysis\n",
    "def create_performance_insights():\n",
    "    \"\"\"Create advanced performance analysis plots\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Overfitting Analysis', 'Training Efficiency',\n",
    "                       'Convergence Rate', 'Performance Metrics Heatmap')\n",
    "    )\n",
    "    \n",
    "    epochs = training_metrics['epoch']\n",
    "    \n",
    "    # 1. Overfitting Analysis (Gap between train and val)\n",
    "    train_val_gap = [t - v for t, v in zip(training_metrics['train_acc'], training_metrics['val_acc'])]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=train_val_gap, \n",
    "                  name='Train-Val Gap', line=dict(color='red'),\n",
    "                  fill='tozeroy', fillcolor='rgba(255,0,0,0.2)'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Training Efficiency (Accuracy improvement per time)\n",
    "    acc_improvement = [training_metrics['val_acc'][i] - training_metrics['val_acc'][0] \n",
    "                      for i in range(len(training_metrics['val_acc']))]\n",
    "    efficiency = [acc / time if time > 0 else 0 \n",
    "                 for acc, time in zip(acc_improvement, training_metrics['epoch_time'])]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=efficiency, \n",
    "                  name='Training Efficiency', line=dict(color='green')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Convergence Rate (Loss reduction rate)\n",
    "    loss_reduction = [training_metrics['val_loss'][0] - loss \n",
    "                     for loss in training_metrics['val_loss']]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=loss_reduction, \n",
    "                  name='Loss Reduction', line=dict(color='blue')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Performance Metrics Correlation Heatmap\n",
    "    import pandas as pd\n",
    "    metrics_df = pd.DataFrame(training_metrics)\n",
    "    correlation_matrix = metrics_df[['train_loss', 'val_loss', 'train_acc', 'val_acc', \n",
    "                                   'gradient_norm', 'weight_norm', 'epoch_time']].corr()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=correlation_matrix.values,\n",
    "                  x=correlation_matrix.columns,\n",
    "                  y=correlation_matrix.columns,\n",
    "                  colorscale='RdBu',\n",
    "                  text=correlation_matrix.values,\n",
    "                  texttemplate='%{text:.2f}',\n",
    "                  textfont={\"size\": 10}),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Advanced Performance Analysis\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update axis titles\n",
    "    fig.update_yaxes(title_text=\"Accuracy Gap\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Acc/Time\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Loss Reduction\", row=2, col=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display performance insights\n",
    "performance_fig = create_performance_insights()\n",
    "performance_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Evaluation and Visualization\n",
    "def evaluate_final_model():\n",
    "    \"\"\"Comprehensive final model evaluation\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    class_correct = [0] * 10\n",
    "    class_total = [0] * 10\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracies = [100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0 \n",
    "                       for i in range(10)]\n",
    "    \n",
    "    # Create final evaluation plots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Final Confusion Matrix', 'Per-Class Accuracy',\n",
    "                       'Training Progress Summary', 'Model Performance Radar'),\n",
    "        specs=[[{\"type\": \"heatmap\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"scatterpolar\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Final Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=cm, x=class_names, y=class_names,\n",
    "                  colorscale='Blues', text=cm, texttemplate='%{text}'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Per-Class Accuracy\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=class_names, y=class_accuracies,\n",
    "               marker_color='lightgreen',\n",
    "               text=[f'{acc:.1f}%' for acc in class_accuracies],\n",
    "               textposition='auto'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Training Progress Summary\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=training_metrics['epoch'], y=training_metrics['train_loss'],\n",
    "                  name='Train Loss', line=dict(color='red')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=training_metrics['epoch'], y=training_metrics['val_loss'],\n",
    "                  name='Val Loss', line=dict(color='blue')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Model Performance Radar Chart\n",
    "    final_metrics = {\n",
    "        'Final Val Accuracy': training_metrics['val_acc'][-1] * 100,\n",
    "        'Training Stability': 100 - (max(training_metrics['val_loss']) - min(training_metrics['val_loss'])) * 50,\n",
    "        'Convergence Speed': (1 - training_metrics['val_loss'][-1] / training_metrics['val_loss'][0]) * 100,\n",
    "        'Generalization': 100 - abs(training_metrics['train_acc'][-1] - training_metrics['val_acc'][-1]) * 100,\n",
    "        'Parameter Efficiency': min(100, (training_metrics['val_acc'][-1] * 1000000) / total_params)\n",
    "    }\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatterpolar(r=list(final_metrics.values()),\n",
    "                       theta=list(final_metrics.keys()),\n",
    "                       fill='toself',\n",
    "                       name='Model Performance'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=1000,\n",
    "        title_text=\"Final Model Evaluation Dashboard\"\n",
    "    )\n",
    "    \n",
    "    return fig, class_accuracies\n",
    "\n",
    "# Create final evaluation\n",
    "final_fig, class_accs = evaluate_final_model()\n",
    "final_fig.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\n=== FINAL MODEL EVALUATION ===\")\n",
    "print(f\"Overall Test Accuracy: {100 * sum(class_accs) / len(class_accs):.2f}%\")\n",
    "print(\"\\nPer-Class Accuracies:\")\n",
    "for i, (class_name, acc) in enumerate(zip(class_names, class_accs)):\n",
    "    print(f\"{class_name:12}: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Training Results and Generate Report\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_training_report():\n",
    "    \"\"\"Save comprehensive training report\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'model_info': {\n",
    "            'architecture': 'AlexNet',\n",
    "            'dataset': 'CIFAR-10',\n",
    "            'total_parameters': total_param,\n",
    "            # 'trainable_parameters': trainable_params\n",
    "        },\n",
    "        'training_config': {\n",
    "            'num_epochs': num_epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'optimizer': 'SGD',\n",
    "            'momentum': 0.9,\n",
    "            'weight_decay': 0.005\n",
    "        },\n",
    "        'final_metrics': {\n",
    "            'final_train_accuracy': float(training_metrics['train_acc'][-1]),\n",
    "            'final_val_accuracy': float(training_metrics['val_acc'][-1]),\n",
    "            'final_train_loss': float(training_metrics['train_loss'][-1]),\n",
    "            'final_val_loss': float(training_metrics['val_loss'][-1]),\n",
    "            'best_val_accuracy': float(max(training_metrics['val_acc'])),\n",
    "            'best_val_epoch': int(training_metrics['val_acc'].index(max(training_metrics['val_acc'])) + 1)\n",
    "        },\n",
    "        'training_metrics': training_metrics,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    report_path = f\"alexnet_training_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"Training report saved to: {report_path}\")\n",
    "    return report_path\n",
    "\n",
    "# Save the report\n",
    "report_path = save_training_report()\n",
    "\n",
    "print(\"\\n=== TRAINING COMPLETE ===\")\n",
    "print(f\"TensorBoard logs: {log_dir}\")\n",
    "print(f\"Training report: {report_path}\")\n",
    "print(\"\\nTo view TensorBoard:\")\n",
    "print(f\"tensorboard --logdir={log_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
